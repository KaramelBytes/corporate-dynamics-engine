"""Data models for the AI integration layer.

This module defines the core data structures used across the AI integration components,
following the specification in ai_integration_spec.md.

Key components:
- AI request and response models
- Configuration models
- Result and status models for various operations
"""
from __future__ import annotations

from enum import Enum
from typing import Any, Dict, List, Optional, Set, Tuple, Union

from pydantic import BaseModel, Field, field_validator


class ContentType(str, Enum):
    """Types of content that can be generated by AI services."""

    SCENARIO = "scenario"
    DIALOGUE = "dialogue"
    CONSEQUENCE = "consequence"
    RELATIONSHIP = "relationship"
    SUMMARY = "summary"


class ProviderType(str, Enum):
    """Supported AI service providers."""

    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GEMINI = "gemini"
    TEMPLATE = "template"  # Fallback provider


class AIConfig(BaseModel):
    """Configuration for AI services.
    
    Contains API keys and settings for all supported providers.
    """

    openai_api_key: Optional[str] = None
    anthropic_api_key: Optional[str] = None
    gemini_api_key: Optional[str] = None
    
    # Budget constraints
    monthly_budget: float = 10.0  # Default $10 monthly budget
    cost_per_1k_tokens: Dict[str, float] = Field(
        default_factory=lambda: {
            "openai": 0.002,  # $0.002 per 1K tokens
            "anthropic": 0.0015,  # $0.0015 per 1K tokens
            "gemini": 0.0005,  # $0.0005 per 1K tokens
        }
    )
    
    # Performance settings
    enable_caching: bool = True
    cache_ttl_hours: int = 24
    similarity_threshold: float = 0.85
    
    # Circuit breaker settings
    error_threshold: int = 3
    reset_timeout_seconds: int = 300
    
    model_config = {"arbitrary_types_allowed": True}


class AIRequest(BaseModel):
    """Base class for AI service requests."""

    content_type: ContentType
    prompt_template: str
    context: Dict[str, Any]
    provider_preference: Optional[ProviderType] = None
    provider_type: Optional[Any] = None  # The actual provider type selected by the orchestrator
    model: Optional[str] = None  # Added model attribute for specifying which model to use
    max_tokens: int = 1000
    temperature: float = 0.7
    top_p: float = 1.0  # Added top_p parameter for nucleus sampling
    stream: bool = False  # Whether to stream the response (required for cost estimation)
    max_cost: Optional[float] = None
    request_id: Optional[str] = None
    prompt: Optional[str] = None  # The final formatted prompt after applying template
    response_format: Optional[Dict[str, Any]] = None  # Format specification for structured responses
    estimated_tokens: Optional[int] = None  # Estimated token count for the request

    model_config = {"arbitrary_types_allowed": True}


class OptimizedAIRequest(BaseModel):
    """AI request with optimization applied.
    
    Created by the optimization process from a base AIRequest.
    """

    original_request: AIRequest
    optimized_prompt: str
    compressed_context: Dict[str, Any]
    estimated_cost: float
    optimization_strategy: str

    model_config = {"arbitrary_types_allowed": True}


class AIResponse(BaseModel):
    """Base class for AI service responses."""

    content_type: ContentType
    content: Any
    provider: ProviderType
    tokens_used: int
    cost: float
    request_id: str
    cache_hit: bool = False
    quality_score: float = 1.0
    enhanced: bool = False

    model_config = {"arbitrary_types_allowed": True}


class ScenarioResponse(AIResponse):
    """AI-generated scenario response."""

    content: Dict[str, Any]


class DialogueResponse(AIResponse):
    """AI-generated dialogue response."""

    content: Dict[str, Any]


class CacheResult(BaseModel):
    """Result from a cache operation."""

    cache_hit: bool
    response: Optional[AIResponse] = None
    cost_saved: float = 0.0
    adaptation_required: str = "none"  # none, minor, moderate, major


class QualityResult(BaseModel):
    """Result from quality validation and enhancement."""

    authenticity_score: float = 1.0
    safety_score: float = 1.0
    consistency_score: float = 1.0
    overall_quality_score: float = 1.0
    
    authenticity_issues: List[str] = Field(default_factory=list)
    safety_issues: List[str] = Field(default_factory=list)
    consistency_issues: List[str] = Field(default_factory=list)
    
    enhanced_response: Optional[AIResponse] = None
    enhancement_applied: bool = False


class OptimizationResult(BaseModel):
    """Result from optimization process.
    
    This is a generic wrapper for various optimization results.
    """
    
    optimized_request: Optional[AIRequest] = None
    estimated_cost_savings: float = 0.0
    optimization_applied: bool = False
    optimization_type: str = "none"


class CostOptimizationResult(BaseModel):
    """Result from cost optimization process."""

    baseline_cost: float = 0.0
    optimized_request: Optional[AIRequest] = None
    optimization_strategy: str = "none"
    estimated_quality_impact: float = 0.0
    final_estimated_cost: float = 0.0
    
    token_reduction: int = 0
    context_compression_ratio: float = 1.0
    savings_percentage: float = 0.0


class ValidationContext(BaseModel):
    """Context for validating AI responses."""

    content_type: ContentType
    original_request: AIRequest
    corporate_profile: Dict[str, Any]
    stakeholder_profiles: Dict[str, Dict[str, Any]]
    expected_format: Optional[Dict[str, Any]] = None


class CircuitBreakerStatus(BaseModel):
    """Status of a circuit breaker for an AI provider."""

    provider: ProviderType
    is_open: bool = False
    failure_count: int = 0
    last_failure_time: Optional[float] = None
    will_reset_at: Optional[float] = None
